# Claude Model & Document Chunking System - Implementation Guide

## ğŸ¯ What Was Implemented

### 1. **Correct Claude Model Configuration**

Updated the system with the **correct Claude 3.5 Sonnet specifications**:

- **Model ID**: `claude-3-5-sonnet-20241022` (was incorrectly `claude-sonnet-4-20250514`)
- **Max Context Window**: **200,000 tokens** (can read ~150-200 pages)
- **Max Output Tokens**: **8,192 tokens** (was 4,096)
- **Safe Chunk Size**: **95,000 tokens** (under 100k limit)

### 2. **Document Chunking System**

Created an intelligent document chunking service that:
- âœ… Automatically splits documents >95k tokens into manageable chunks
- âœ… Stays under 100k token limit per request (even though model supports 200k)
- âœ… Splits on natural boundaries (paragraphs â†’ sentences â†’ characters)
- âœ… Deduplicates results across chunks
- âœ… Logs processing for transparency

### 3. **Updated Admin Settings Interface**

The AI Settings page now shows:
- **Claude 3.5 Sonnet** (Recommended) - 200K context
- **Claude 3.5 Haiku** - 200K context (Faster)
- **Claude 3 Opus** - 200K context (Most Capable)
- Correct max output tokens (8,192)
- Context about 200k input capacity

---

## ğŸ“Š Technical Specifications

### **Claude 3.5 Sonnet**
```
Model: claude-3-5-sonnet-20241022
Input Tokens: 200,000 (context window)
Output Tokens: 8,192 (max response)
Cost: ~$3 per million input tokens, ~$15 per million output tokens
```

### **Token Estimation**
- **1 token â‰ˆ 4 characters** (English text average)
- **100,000 tokens â‰ˆ 400,000 characters â‰ˆ 75,000 words**
- **Safe chunk size: 95,000 tokens** (buffer for system prompts)

---

## ğŸ—ï¸ System Architecture

### **Document Processing Flow**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Upload PDF/    â”‚
â”‚  DOCX/TXT/XLSX  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Extract Text    â”‚
â”‚ from Document   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Estimate Tokens â”‚â—„â”€â”€â”€â”€ DocumentChunker
â”‚ (chars Ã· 4)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
    â”‚ < 95k?   â”‚
    â””â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”˜
  YES  â”‚   â”‚ NO
       â”‚   â”‚
       â”‚   â–¼
       â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚ â”‚ Split into      â”‚
       â”‚ â”‚ Chunks < 95k    â”‚
       â”‚ â”‚ (paragraphs,    â”‚
       â”‚ â”‚  sentences)     â”‚
       â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚          â”‚
       â”‚          â–¼
       â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚    â”‚ Process     â”‚
       â”‚    â”‚ Each Chunk  â”‚
       â”‚    â”‚ Separately  â”‚
       â”‚    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚           â”‚
       â”‚           â–¼
       â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚    â”‚ Deduplicate  â”‚
       â”‚    â”‚ & Merge      â”‚
       â”‚    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚           â”‚
       â–¼           â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Send to Claude   â”‚
    â”‚ API (< 100k)     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Extract Customs  â”‚
    â”‚ Categories       â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Update Database  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Files Created/Modified

### **New Files**
```
app/Services/DocumentChunker.php          # Intelligent document chunking service
CLAUDE_MODEL_UPGRADE_GUIDE.md             # This documentation
```

### **Modified Files**
```
config/services.php                       # Updated Claude config
app/Services/LawDocumentProcessor.php     # Added chunking support
app/Services/ItemClassifier.php           # Added chunking support
resources/views/admin/settings/index.blade.php  # Updated model options
```

---

## ğŸ”§ Configuration Details

### **config/services.php**
```php
'claude' => [
    'api_key' => env('CLAUDE_API_KEY'),
    'model' => env('CLAUDE_MODEL', 'claude-3-5-sonnet-20241022'),
    'max_tokens' => env('CLAUDE_MAX_TOKENS', 8192),
    'max_context_tokens' => env('CLAUDE_MAX_CONTEXT_TOKENS', 200000),
    'chunk_size' => env('CLAUDE_CHUNK_SIZE', 95000), // Safe size under 100k
],
```

### **.env Configuration**
```env
CLAUDE_API_KEY=sk-ant-your-key-here
CLAUDE_MODEL=claude-3-5-sonnet-20241022
CLAUDE_MAX_TOKENS=8192
CLAUDE_MAX_CONTEXT_TOKENS=200000
CLAUDE_CHUNK_SIZE=95000
```

---

## ğŸš€ How Document Chunking Works

### **DocumentChunker Service**

#### **1. Token Estimation**
```php
$estimatedTokens = $chunker->estimateTokens($text);
// Uses 4 characters per token rule
```

#### **2. Smart Splitting**
The chunker splits documents intelligently:
1. **First**: Try to split on paragraph breaks (`\n\n`)
2. **Second**: If paragraphs too large, split on sentences (`.!?`)
3. **Last Resort**: Force split by character count

#### **3. Chunk Processing**
```php
$results = $chunker->processInChunks(
    $text,
    function($chunk, $index, $total) {
        // Process each chunk
        return processChunk($chunk);
    },
    function($results) {
        // Merge results from all chunks
        return mergeResults($results);
    }
);
```

#### **4. Overlap Support** (Optional)
```php
$chunksWithOverlap = $chunker->chunkWithOverlap($text);
// Adds 5% overlap between chunks for context preservation
```

---

## ğŸ“ Usage Examples

### **Example 1: Processing Large Law Document**

```php
// Automatically handled by LawDocumentProcessor
$processor = new LawDocumentProcessor(new DocumentChunker());
$result = $processor->process($lawDocument);

// If document > 95k tokens:
// - Automatically split into chunks
// - Each chunk processed separately
// - Results merged and deduplicated
```

**Example Log Output:**
```
[INFO] Chunking document
  - total_estimated_tokens: 250000
  - max_tokens_per_chunk: 95000
  - estimated_chunks: 3

[INFO] Processing chunk 1 of 3
[INFO] Processing chunk 2 of 3
[INFO] Processing chunk 3 of 3

[INFO] Chunked processing complete
  - total_chunks: 3
  - unique_categories: 847
```

### **Example 2: Manual Chunking**

```php
$chunker = new DocumentChunker();

// Simple chunking
$chunks = $chunker->chunkText($largeText);

// Chunking with custom size
$chunks = $chunker->chunkText($largeText, 50000);

// Chunking with overlap
$chunks = $chunker->chunkWithOverlap($largeText, 95000, 5000);

// Get token estimate
$tokens = $chunker->estimateTokens($text);
```

---

## ğŸ¯ Why 95k Instead of 200k?

Even though Claude 3.5 Sonnet supports **200,000 token input**, we chunk at **95,000 tokens** because:

### **Advantages:**

1. **âœ… Cost Control**
   - Each request costs based on tokens used
   - 100k token request = ~$0.30
   - Multiple smaller requests give better control

2. **âœ… Faster Processing**
   - Smaller chunks = faster responses
   - Can process chunks in parallel (future enhancement)
   - Better user experience

3. **âœ… Better Error Recovery**
   - If one chunk fails, others still process
   - Easier to retry failed chunks
   - More robust system

4. **âœ… Memory Management**
   - Large responses can cause memory issues
   - Smaller chunks easier to handle
   - Better for server resources

5. **âœ… Deduplication**
   - Breaking into chunks allows deduplication
   - Prevents duplicate customs codes
   - Cleaner results

### **When to Use Full 200k:**

You CAN increase `CLAUDE_CHUNK_SIZE` to 180000+ for:
- Very structured documents where deduplication isn't needed
- Priority on processing speed over cost
- Documents with sequential, non-repetitive content

---

## ğŸ” Monitoring & Logging

### **What Gets Logged:**

```php
// Document size estimation
Log::info('Processing document in single request', [
    'estimated_tokens' => 45000
]);

// Chunking decision
Log::info('Document exceeds token limit, chunking', [
    'estimated_tokens' => 150000,
    'max_safe_tokens' => 95000
]);

// Chunk processing
Log::info('Processing chunk', [
    'chunk_number' => 2,
    'total_chunks' => 3
]);

// Completion
Log::info('Chunked processing complete', [
    'total_chunks' => 3,
    'unique_categories' => 847
]);
```

### **Check Logs:**
```bash
tail -f storage/logs/laravel.log | grep "Processing"
```

---

## ğŸ§ª Testing

### **Test Small Document (< 95k tokens)**
```php
// Should process in single request
$text = str_repeat("Test content. ", 5000); // ~20k tokens
$processor->process($document);
// Check logs: "Processing document in single request"
```

### **Test Large Document (> 95k tokens)**
```php
// Should chunk automatically
$text = str_repeat("Test content. ", 50000); // ~200k tokens
$processor->process($document);
// Check logs: "Document exceeds token limit, chunking"
// Check logs: "Processing chunk X of Y"
```

### **Test Token Estimation**
```php
$chunker = new DocumentChunker();

$text = "This is a test."; // 15 chars
$tokens = $chunker->estimateTokens($text);
// Result: ~4 tokens (15 Ã· 4)
```

---

## âš™ï¸ Configuration Options

### **Adjust Chunk Size**

In `.env`:
```env
# Conservative (recommended for cost control)
CLAUDE_CHUNK_SIZE=95000

# Moderate
CLAUDE_CHUNK_SIZE=150000

# Aggressive (use full context, higher cost)
CLAUDE_CHUNK_SIZE=180000
```

### **Adjust Output Tokens**

```env
# Conservative
CLAUDE_MAX_TOKENS=4096

# Standard (recommended)
CLAUDE_MAX_TOKENS=8192

# Maximum (for very detailed responses)
CLAUDE_MAX_TOKENS=8192  # This is the hard limit
```

---

## ğŸ“Š Performance Expectations

### **Processing Times** (approximate)

| Document Size | Token Count | Chunks | Processing Time |
|--------------|-------------|---------|-----------------|
| 10 pages     | 15,000      | 1       | 5-10 seconds    |
| 50 pages     | 75,000      | 1       | 15-30 seconds   |
| 100 pages    | 150,000     | 2       | 30-60 seconds   |
| 200 pages    | 300,000     | 4       | 60-120 seconds  |

### **Cost Estimates**

| Tokens | Input Cost | Output Cost (8k) | Total  |
|--------|-----------|------------------|--------|
| 50k    | $0.15     | $0.12            | $0.27  |
| 100k   | $0.30     | $0.12            | $0.42  |
| 200k   | $0.60     | $0.12            | $0.72  |

---

## ğŸ“ Best Practices

### **1. Use Appropriate Chunk Sizes**
```php
// For law documents (lots of repetition)
CLAUDE_CHUNK_SIZE=95000  âœ… Good

// For technical manuals (sequential)
CLAUDE_CHUNK_SIZE=150000  âœ… Good
```

### **2. Monitor Your Usage**
- Check logs regularly
- Track processing times
- Monitor API costs in Anthropic Console

### **3. Cache When Possible**
```php
// ItemClassifier already caches customs codes
$classifier->clearCache($countryId);  // Call after updates
```

### **4. Test with Real Documents**
- Upload sample PDFs of varying sizes
- Check logs for chunking behavior
- Verify results accuracy

---

## ğŸ†˜ Troubleshooting

### **Problem: "Token limit exceeded" error**

**Solution**: Decrease chunk size
```env
CLAUDE_CHUNK_SIZE=80000
```

### **Problem: Processing too slow**

**Solution**: Increase chunk size (trade cost for speed)
```env
CLAUDE_CHUNK_SIZE=150000
```

### **Problem: Duplicate categories**

**Solution**: Chunking already deduplicates, but if still seeing duplicates:
```php
// Check LawDocumentProcessor::processInChunks()
// Deduplication logic is already implemented
```

### **Problem: Memory issues with large files**

**Solution**: 
1. Decrease chunk size
2. Check PHP memory limit: `memory_limit=512M` in `php.ini`
3. Process files in background queue

---

## ğŸ“ˆ Future Enhancements

### **Potential Improvements:**

1. **Parallel Processing**
   ```php
   // Process chunks simultaneously
   Queue::bulk($chunks->map(fn($c) => new ProcessChunk($c)));
   ```

2. **Adaptive Chunking**
   ```php
   // Adjust chunk size based on document complexity
   $chunkSize = $this->calculateOptimalChunkSize($text);
   ```

3. **Caching Results**
   ```php
   // Cache processed documents by hash
   $hash = hash('sha256', $text);
   Cache::remember("doc_{$hash}", ...);
   ```

4. **Progress Tracking**
   ```php
   // Real-time progress updates
   event(new ChunkProcessed($index, $total));
   ```

---

## âœ… Summary

**What Changed:**
- âœ… Updated to correct Claude 3.5 Sonnet model (`claude-3-5-sonnet-20241022`)
- âœ… Correct token limits (200k input, 8k output)
- âœ… Intelligent document chunking under 100k tokens
- âœ… Automatic deduplication
- âœ… Smart splitting (paragraphs â†’ sentences â†’ chars)
- âœ… Comprehensive logging

**Benefits:**
- ğŸ’° Better cost control
- âš¡ Faster processing
- ğŸ›¡ï¸ More robust error handling
- ğŸ“Š Better monitoring
- ğŸ¯ Accurate results even with huge documents

**Ready to Use:**
- Configure Claude API key in Admin â†’ AI Settings
- Upload documents of any size
- System automatically handles chunking
- Monitor logs for processing details

---

**Updated:** January 22, 2026
**Version:** 1.0
